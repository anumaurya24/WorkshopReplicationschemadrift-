{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f11a148c-b863-45b9-8bfa-e54310dbdac0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "from pyspark import pipelines as pl\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    MapType, StringType, DoubleType, DateType\n",
    ")\n",
    "\n",
    "\n",
    "# Path where JSON files land\n",
    "volume_path = \"/Volumes/workspace/schema_drift/json\"\n",
    "\n",
    "# Define datatype fixes needed in silver\n",
    "updated_datatypes = {\n",
    "    \"signupDate\": \"date\",\n",
    "    \"CreditScore\": \"double\"\n",
    "}\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "# BRONZE TABLE\n",
    "pl.create_streaming_table(\"demo_cust_bronze_sd\")\n",
    "\n",
    "@pl.append_flow(\n",
    "    target=\"demo_cust_bronze_sd\",\n",
    "    name=\"demo_cust_bronze_sd_ingest_flow\"\n",
    ")\n",
    "def demo_cust_bronze_sd_ingest_flow():\n",
    "    df = (\n",
    "        spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"json\")\n",
    "            .option(\"cloudFiles.inferColumnTypes\", \"false\")\n",
    "            .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")\n",
    "            .option(\"rescuedDataColumn\", \"_rescued_data\")\n",
    "            .load(volume_path)\n",
    "    )\n",
    "\n",
    "    # Add lineage & ingestion metadata\n",
    "    df = (\n",
    "        df.withColumn(\"ingestion_datetime\", F.current_timestamp())\n",
    "          .withColumn(\"source_filename\", F.col(\"_metadata.file_path\"))\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "# RESCUED DATA FIELD EXTRACTOR (FIXED VERSION)\n",
    "\n",
    "def process__rescue_data_new_fields(df):\n",
    "    \"\"\"\n",
    "    Safely extract new fields out of `_rescued_data` JSON STRING into real columns.\n",
    "    Streaming-safe: no collect(), no toPandas().\n",
    "    \"\"\"\n",
    "    # These are the new fields present in customer_data_2,3,4\n",
    "    expected_fields = [\"Age\", \"Gender\", \"LoyaltyStatus\", \"CreditScore\"]\n",
    "\n",
    "    if \"_rescued_data\" in df.columns:\n",
    "\n",
    "        # Convert STRING â†’ MAP\n",
    "        df = df.withColumn(\n",
    "            \"_rescued_map\",\n",
    "            F.from_json(F.col(\"_rescued_data\"), MapType(StringType(), StringType()))\n",
    "        )\n",
    "\n",
    "        # Extract each expected key (if it doesn't already exist)\n",
    "        for key in expected_fields:\n",
    "            if key not in df.columns:\n",
    "                df = df.withColumn(\n",
    "                    key,\n",
    "                    F.col(\"_rescued_map\").getItem(key)\n",
    "                )\n",
    "\n",
    "        # Drop helper column\n",
    "        df = df.drop(\"_rescued_map\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "# DATATYPE FIXER\n",
    "\n",
    "def process__rescue_data_datatype_change(df, target_schema_map: dict):\n",
    "    \"\"\"\n",
    "    Adjust datatypes after rescued fields are created.\n",
    "    Example: {\"signupDate\": \"date\", \"CreditScore\": \"double\"}\n",
    "    \"\"\"\n",
    "    for col_name, target_type in target_schema_map.items():\n",
    "\n",
    "        if col_name not in df.columns:\n",
    "            continue  # don't fail on missing fields\n",
    "\n",
    "        if target_type.lower() == \"date\":\n",
    "            df = df.withColumn(col_name, F.to_date(F.col(col_name)))\n",
    "\n",
    "        elif target_type.lower() in (\"double\", \"float\"):\n",
    "            df = df.withColumn(col_name, F.col(col_name).cast(\"double\"))\n",
    "\n",
    "        elif target_type.lower() in (\"int\", \"integer\"):\n",
    "            df = df.withColumn(col_name, F.col(col_name).cast(\"int\"))\n",
    "\n",
    "        elif target_type.lower() in (\"long\", \"bigint\"):\n",
    "            df = df.withColumn(col_name, F.col(col_name).cast(\"bigint\"))\n",
    "\n",
    "        else:\n",
    "            df = df.withColumn(col_name, F.col(col_name).cast(\"string\"))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "# SILVER TABLE\n",
    "pl.create_streaming_table(\"demo_cust_silver_sd\")\n",
    "\n",
    "@pl.append_flow(\n",
    "    target=\"demo_cust_silver_sd\",\n",
    "    name=\"demo_cust_silver_sd_clean_flow\"\n",
    ")\n",
    "def demo_cust_silver_sd_clean_flow():\n",
    "    df = spark.readStream.table(\"demo_cust_bronze_sd\")\n",
    "    df = process__rescue_data_new_fields(df)\n",
    "    df = process__rescue_data_datatype_change(df, updated_datatypes)\n",
    "    return df\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Schema_Drift",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
