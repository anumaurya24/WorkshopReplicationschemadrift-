{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb48e973-5fd4-46f6-a0fc-45615929bae5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import pipelines as pl\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    MapType,\n",
    "    StringType,\n",
    "    DoubleType,\n",
    "    DateType,\n",
    "    IntegerType,\n",
    "    StructType,\n",
    "    StructField,\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# CONFIG\n",
    "# ============================================\n",
    "\n",
    "# Path where JSON files land in schema_drift_fresh volume\n",
    "# (pattern assumes files like customer_data_1.json, customer_data_2.json, ...)\n",
    "volume_path = \"/Volumes/workspace/damg7370/datastore/schema_drift_fresh/customer_data_*.json\"\n",
    "\n",
    "# Define ONLY the base schema from customer_data_1.json\n",
    "base_schema = StructType(\n",
    "    [\n",
    "        StructField(\"CustomerID\", StringType(), True),\n",
    "        StructField(\"FullName\", StringType(), True),\n",
    "        StructField(\"Email\", StringType(), True),\n",
    "        StructField(\"PhoneNumber\", StringType(), True),\n",
    "        StructField(\"City\", StringType(), True),\n",
    "        StructField(\"SignupDate\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define datatype fixes needed in silver\n",
    "# (what final schema you want in Silver)\n",
    "updated_datatypes = StructType(\n",
    "    [\n",
    "        StructField(\"SignupDate\", DateType(), True),\n",
    "        StructField(\"Age\", IntegerType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# BRONZE TABLE - STREAMING (RESCUE MODE)\n",
    "# ============================================\n",
    "\n",
    "# IMPORTANT: use ONLY the table name here.\n",
    "# The catalog+schema (e.g. damg7370.bronze) come from the pipeline \"Target\" setting.\n",
    "pl.create_streaming_table(\"demo_cust_bronze_rescue\")\n",
    "\n",
    "\n",
    "@pl.append_flow(\n",
    "    target=\"demo_cust_bronze_rescue\",\n",
    "    name=\"demo_cust_bronze_rescue_ingest_flow\",\n",
    ")\n",
    "def demo_cust_bronze_rescue_ingest_flow():\n",
    "    \"\"\"\n",
    "    Bronze layer with RESCUE mode and LOCKED base_schema.\n",
    "    \"\"\"\n",
    "    df = (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"json\")\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"false\")\n",
    "        .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")\n",
    "        .option(\"rescuedDataColumn\", \"_rescued_data\")\n",
    "        .schema(base_schema)\n",
    "        .load(volume_path)\n",
    "    )\n",
    "\n",
    "    df = (\n",
    "        df.withColumn(\"ingestion_datetime\", F.current_timestamp())\n",
    "        .withColumn(\"source_filename\", F.col(\"_metadata.file_path\"))\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# HELPERS\n",
    "# ============================================\n",
    "\n",
    "def process__rescue_data_datatype_change(df, target_schema: StructType):\n",
    "    \"\"\"\n",
    "    Adjust datatypes after rescued fields are created,\n",
    "    using the target_schema definition.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"_rescued_data_modified\",\n",
    "        F.from_json(F.col(\"_rescued_data\"), MapType(StringType(), StringType())),\n",
    "    )\n",
    "\n",
    "    for field in target_schema.fields:\n",
    "        data_type = field.dataType\n",
    "        column_name = field.name\n",
    "\n",
    "        key_condition = F.expr(\n",
    "            f\"_rescued_data_modified IS NOT NULL \"\n",
    "            f\"AND map_contains_key(_rescued_data_modified, '{column_name}')\"\n",
    "        )\n",
    "\n",
    "        rescued_value = F.when(\n",
    "            key_condition,\n",
    "            F.col(\"_rescued_data_modified\").getItem(column_name).cast(data_type),\n",
    "        ).otherwise(F.col(column_name).cast(data_type))\n",
    "\n",
    "        df = df.withColumn(column_name, rescued_value)\n",
    "        df = df.withColumn(column_name, F.col(column_name).cast(data_type))\n",
    "\n",
    "    df = df.drop(\"_rescued_data_modified\")\n",
    "    df = df.withColumn(\"_rescued_data\", F.lit(None).cast(StringType()))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def process__rescue_data_new_fields(df):\n",
    "    \"\"\"\n",
    "    Extract new fields from _rescued_data.\n",
    "    Uses a hardcoded list of the fields we expect in customer_data_2/3/4.\n",
    "    \"\"\"\n",
    "\n",
    "    # Hardcode the fields we know can appear via schema drift\n",
    "    expected_fields = [\"Age\", \"Gender\", \"LoyaltyStatus\", \"CreditScore\"]\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"_rescued_data_json_to_map\",\n",
    "        F.from_json(F.col(\"_rescued_data\"), MapType(StringType(), StringType())),\n",
    "    )\n",
    "\n",
    "    for key in expected_fields:\n",
    "        if key not in df.columns:\n",
    "            df = df.withColumn(\n",
    "                key, F.col(\"_rescued_data_json_to_map\").getItem(key)\n",
    "            )\n",
    "\n",
    "    df = df.drop(\"_rescued_data_json_to_map\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# SILVER TABLE - STREAMING\n",
    "# ============================================\n",
    "\n",
    "pl.create_streaming_table(\n",
    "    name=\"demo_cust_silver_rescue\",\n",
    "    expect_all_or_drop={\n",
    "        \"no_rescued_data\": \"_rescued_data IS NULL\",\n",
    "        \"valid_id\": \"CustomerID IS NOT NULL\",\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "@pl.append_flow(\n",
    "    target=\"demo_cust_silver_rescue\",\n",
    "    name=\"demo_cust_silver_rescue_clean_flow\",\n",
    ")\n",
    "def demo_cust_silver_rescue_clean_flow():\n",
    "    # Read from Bronze (same table name as above)\n",
    "    df = spark.readStream.table(\"demo_cust_bronze_rescue\")\n",
    "\n",
    "    # 1) Handle new fields arriving in _rescued_data\n",
    "    df = process__rescue_data_new_fields(df)\n",
    "\n",
    "    # 2) Fix datatypes (e.g., SignupDate -> DATE, Age -> INT)\n",
    "    df = process__rescue_data_datatype_change(df, updated_datatypes)\n",
    "\n",
    "    return df\n",
    "\n"
    "    return df\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pipeline-1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
