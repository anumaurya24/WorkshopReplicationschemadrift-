{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea0847fb-6361-4494-a74c-6c5df6a410c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import pipelines as pl\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    MapType, StringType, DateType, IntegerType, StructType, StructField\n",
    ")\n",
    "\n",
    "volume_path = \"/Volumes/workspace/new_schema_drift/json_new/Customer_New_Json/*.json\"\n",
    "\n",
    "base_schema = StructType([\n",
    "    StructField(\"CustomerID\", StringType(), True),\n",
    "    StructField(\"FullName\", StringType(), True),\n",
    "    StructField(\"Email\", StringType(), True),\n",
    "    StructField(\"PhoneNumber\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"SignupDate\", StringType(), True)\n",
    "])\n",
    "\n",
    "updated_datatypes = StructType([\n",
    "    StructField(\"SignupDate\", DateType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# BRONZE TABLE\n",
    "pl.create_streaming_table(\"workspace.new_schema_drift.demo_cust_bronze_rescue\")\n",
    "\n",
    "@pl.append_flow(\n",
    "    target=\"workspace.new_schema_drift.demo_cust_bronze_rescue\",\n",
    "    name=\"demo_cust_bronze_rescue_ingest_flow\"\n",
    ")\n",
    "def demo_cust_bronze_rescue_ingest_flow():\n",
    "    return (\n",
    "        spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"json\")\n",
    "            .option(\"cloudFiles.inferColumnTypes\", \"false\")\n",
    "            .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")\n",
    "            .schema(base_schema)\n",
    "            .load(volume_path)\n",
    "            .withColumn(\"ingestion_datetime\", F.current_timestamp())\n",
    "            .withColumn(\"source_filename\", F.col(\"_metadata.file_path\"))\n",
    "    )\n",
    "\n",
    "def process__rescue_data_datatype_change(df, target_schema):\n",
    "    df = df.withColumn(\n",
    "        \"_rescued_map\",\n",
    "        F.from_json(F.col(\"_rescued_data\"), MapType(StringType(), StringType()))\n",
    "    )\n",
    "\n",
    "    for field in target_schema.fields:\n",
    "        df = df.withColumn(\n",
    "            field.name,\n",
    "            F.when(\n",
    "                F.col(\"_rescued_map\").getItem(field.name).isNotNull(),\n",
    "                F.col(\"_rescued_map\").getItem(field.name).cast(field.dataType)\n",
    "            ).otherwise(F.col(field.name).cast(field.dataType))\n",
    "        )\n",
    "\n",
    "    return df.drop(\"_rescued_map\")\n",
    "\n",
    "\n",
    "def process__rescue_data_new_fields(df):\n",
    "    fields = [\"Age\", \"Gender\", \"LoyaltyStatus\"]\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"_rescued_json\",\n",
    "        F.from_json(F.col(\"_rescued_data\"), MapType(StringType(), StringType()))\n",
    "    )\n",
    "\n",
    "    for col in fields:\n",
    "        if col not in df.columns:\n",
    "            df = df.withColumn(col, F.col(\"_rescued_json\").getItem(col))\n",
    "\n",
    "    return df.drop(\"_rescued_json\")\n",
    "\n",
    "\n",
    "pl.create_streaming_table(\n",
    "    name=\"workspace.new_schema_drift.demo_cust_silver_rescue\",\n",
    "    expect_all_or_drop={\n",
    "        \"rescued_is_null\": \"_rescued_data IS NULL\",\n",
    "        \"valid_id\": \"CustomerID IS NOT NULL\"\n",
    "    }\n",
    ")\n",
    "\n",
    "@pl.append_flow(\n",
    "    target=\"workspace.new_schema_drift.demo_cust_silver_rescue\",\n",
    "    name=\"demo_cust_silver_rescue_clean_flow\"\n",
    ")\n",
    "def demo_cust_silver_rescue_clean_flow():\n",
    "    df = spark.readStream.table(\"workspace.new_schema_drift.demo_cust_bronze_rescue\")\n",
    "    df = process__rescue_data_new_fields(df)\n",
    "    df = process__rescue_data_datatype_change(df, updated_datatypes)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d71aa7ba-9434-426c-a7e7-baf521eced0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import pipelines as pl\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "volume_path = \"/Volumes/workspace/new_schema_drift/json_new/Customer_New_Json/*.json\"\n",
    "\n",
    "# ======================================================\n",
    "# BRONZE - addNewColumns MODE\n",
    "# ======================================================\n",
    "pl.create_streaming_table(\"demo_cust_bronze_addnew\")\n",
    "\n",
    "@pl.append_flow(\n",
    "    target=\"demo_cust_bronze_addnew\",\n",
    "    name=\"demo_cust_bronze_addnew_ingest_flow\"\n",
    ")\n",
    "def demo_cust_bronze_addnew_ingest_flow():\n",
    "    df = (\n",
    "        spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"json\")\n",
    "            .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "            .option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\")\n",
    "            .load(volume_path)\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        df.withColumn(\"ingestion_datetime\", F.current_timestamp())\n",
    "          .withColumn(\"source_filename\", F.col(\"_metadata.file_path\"))\n",
    "    )\n",
    "\n",
    "# ======================================================\n",
    "# SILVER - addNewColumns MODE\n",
    "# ======================================================\n",
    "pl.create_streaming_table(\n",
    "    name=\"demo_cust_silver_addnew\",\n",
    "    expect_all_or_drop={\n",
    "        \"valid_id\": \"CustomerID IS NOT NULL\"\n",
    "    }\n",
    ")\n",
    "\n",
    "@pl.append_flow(\n",
    "    target=\"demo_cust_silver_addnew\",\n",
    "    name=\"demo_cust_silver_addnew_clean_flow\"\n",
    ")\n",
    "def demo_cust_silver_addnew_clean_flow():\n",
    "\n",
    "    # IMPORTANT â€” READ USING SAME NAME AS CREATED\n",
    "    df = spark.readStream.table(\"demo_cust_bronze_addnew\")\n",
    "\n",
    "    # SIMPLE DATATYPE CLEANUP\n",
    "    df = df.withColumn(\"SignupDate\", F.to_date(\"SignupDate\"))\n",
    "\n",
    "    return df\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) Schema_Drift_New",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
